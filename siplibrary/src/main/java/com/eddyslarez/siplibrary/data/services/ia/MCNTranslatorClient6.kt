package com.eddyslarez.siplibrary.data.services.ia

import android.os.Build
import androidx.annotation.RequiresApi
import kotlinx.coroutines.*
import kotlinx.coroutines.channels.Channel
import kotlinx.coroutines.channels.ReceiveChannel
import kotlinx.coroutines.sync.Mutex
import kotlinx.coroutines.sync.withLock
import okhttp3.*
import org.json.JSONArray
import org.json.JSONObject
import java.io.ByteArrayOutputStream
import java.util.*
import java.util.concurrent.TimeUnit
import java.util.concurrent.atomic.AtomicBoolean
import android.util.Log

class MCNTranslatorClient6(
    private val apiKey: String,
    private val model: String = "gpt-4o-realtime-preview-2025-06-03"
) {
    private val client = OkHttpClient.Builder()
        .readTimeout(0, TimeUnit.SECONDS)
        .writeTimeout(15, TimeUnit.SECONDS)
        .connectTimeout(30, TimeUnit.SECONDS)
        .pingInterval(20, TimeUnit.SECONDS)
        .retryOnConnectionFailure(true)
        .build()

    private var webSocket: WebSocket? = null
    private val isConnected = AtomicBoolean(false)
    private val isSessionInitialized = AtomicBoolean(false)
    private val coroutineScope = CoroutineScope(Dispatchers.IO + SupervisorJob())

    // Audio management optimizado
    private val audioBuffer = mutableListOf<ByteArray>()
    private val audioBufferMutex = Mutex()
    private val minAudioChunkSize = 1600 // Reducido para mayor responsividad
    private val maxAudioChunkSize = 10 * 1024 * 1024 // Reducido para evitar timeouts
    private val audioSendInterval = 50L // MÃ¡s frecuente para tiempo real

    // Response channels
    private val audioResponseChannel = Channel<ByteArray>(Channel.UNLIMITED)
    private val transcriptionChannel = Channel<String>(Channel.UNLIMITED)
    private val inputTranscriptionChannel = Channel<String>(Channel.UNLIMITED)
    private val translationChannel = Channel<TranslationResult>(Channel.UNLIMITED)

    // Callbacks
    private var onConnectionStateChanged: ((Boolean) -> Unit)? = null
    private var onError: ((String) -> Unit)? = null
    private var onAudioReceived: ((ByteArray) -> Unit)? = null
    private var onTranslationReceived: ((TranslationResult) -> Unit)? = null
    private var onLanguageDetected: ((DetectedLanguage) -> Unit)? = null

    // Language detection and management
    private var currentSourceLanguage: String = "auto"
    private var isTranslationMode = true
    private var confidenceThreshold = 0.7f

    // Audio timeout mejorado
    private var lastAudioReceiveTime = 0L
    private val audioTimeoutMs = 1500L // Reducido para mayor responsividad

    companion object {
        private const val TAG = "MCNTranslatorClient"
        private const val WEBSOCKET_URL = "wss://api.openai.com/v1/realtime"
    }

    // Buffer para audio reproducible optimizado
    private val playbackBuffer = ByteArrayOutputStream()
    private val playbackMutex = Mutex()

    // Manejo de reconexiÃ³n automÃ¡tica
    private var reconnectAttempts = 0
    private val maxReconnectAttempts = 5
    private var isReconnecting = AtomicBoolean(false)

    // Data classes
    data class TranslationResult(
        val originalText: String,
        val translatedText: String,
        val sourceLanguage: String,
        val targetLanguage: String,
        val confidence: Float,
        val timestamp: Long
    )

    data class DetectedLanguage(
        val language: String,
        val confidence: Float,
        val text: String
    )

    // === INSTRUCCIONES OPTIMIZADAS PARA MEJOR RENDIMIENTO ===
    private val translatorInstructions = """
        Eres un traductor profesional ESPAÃ‘OL-RUSO / RUSO-ESPAÃ‘OL en tiempo real.
        
        REGLAS CRÃTICAS:
        1. DETECTA automÃ¡ticamente el idioma (espaÃ±ol o ruso)
        2. TRADUCE inmediatamente al idioma opuesto
        3. RESPONDE SOLO con la traducciÃ³n, sin explicaciones
        4. MANTÃ‰N el tono y contexto original
        5. SÃ‰ RÃPIDO y PRECISO
        
        EJEMPLOS:
        EspaÃ±ol â†’ Ruso: "Hola, Â¿cÃ³mo estÃ¡s?" â†’ "ÐŸÑ€Ð¸Ð²ÐµÑ‚, ÐºÐ°Ðº Ð´ÐµÐ»Ð°?"
        Ruso â†’ EspaÃ±ol: "Ð¡Ð¿Ð°ÑÐ¸Ð±Ð¾ Ð·Ð° Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒ" â†’ "Gracias por la ayuda"
        
        NUNCA agregues comentarios como "La traducciÃ³n es..." o similares.
        RESPONDE directamente con el texto traducido.
        """.trimIndent()

    // === MÃ‰TODOS DE CONFIGURACIÃ“N ===
    fun setConnectionStateListener(listener: (Boolean) -> Unit) {
        onConnectionStateChanged = listener
    }

    fun setErrorListener(listener: (String) -> Unit) {
        onError = listener
    }

    fun setAudioReceivedListener(listener: (ByteArray) -> Unit) {
        onAudioReceived = listener
    }

    fun setTranslationReceivedListener(listener: (TranslationResult) -> Unit) {
        onTranslationReceived = listener
    }

    fun setLanguageDetectedListener(listener: (DetectedLanguage) -> Unit) {
        onLanguageDetected = listener
    }

    fun setTranslationMode(enabled: Boolean) {
        isTranslationMode = enabled
        Log.d(TAG, "Modo traducciÃ³n: ${if (enabled) "ACTIVADO" else "DESACTIVADO"}")
    }

    fun setSourceLanguage(language: String) {
        currentSourceLanguage = language
        Log.d(TAG, "Idioma fuente establecido: $language")
    }

    fun setConfidenceThreshold(threshold: Float) {
        confidenceThreshold = threshold.coerceIn(0.1f, 1.0f)
        Log.d(TAG, "Umbral de confianza: $confidenceThreshold")
    }

    // === CONEXIÃ“N MEJORADA ===
    @RequiresApi(Build.VERSION_CODES.O)
    suspend fun connect(): Boolean {
        return try {
            if (isConnected.get()) {
                Log.d(TAG, "Ya estÃ¡ conectado")
                return true
            }

            val request = Request.Builder()
                .url("$WEBSOCKET_URL?model=$model")
                .addHeader("Authorization", "Bearer $apiKey")
                .addHeader("OpenAI-Beta", "realtime=v1")
                .addHeader("User-Agent", "MCNTranslator/1.0")
                .build()

            Log.d(TAG, "Conectando Traductor MCN...")
            webSocket = client.newWebSocket(request, createWebSocketListener())

            // Esperar conexiÃ³n con timeout
            var attempts = 0
            while (!isConnected.get() && attempts < 100) {
                delay(50)
                attempts++
            }

            if (isConnected.get()) {
                reconnectAttempts = 0
                startAudioStreamingLoop()
                startAudioTimeoutWatcher()
                Log.d(TAG, "Traductor MCN conectado exitosamente")
                true
            } else {
                Log.e(TAG, "Timeout en conexiÃ³n")
                false
            }
        } catch (e: Exception) {
            Log.e(TAG, "Error de conexiÃ³n: ${e.message}")
            onError?.invoke("Error de conexiÃ³n: ${e.message}")
            false
        }
    }

    private fun createWebSocketListener(): WebSocketListener = object : WebSocketListener() {
        override fun onOpen(webSocket: WebSocket, response: Response) {
            Log.d(TAG, "WebSocket conectado - Status: ${response.code}")
            isConnected.set(true)
            onConnectionStateChanged?.invoke(true)

            coroutineScope.launch {
                try {
                    delay(100) // PequeÃ±a pausa antes de inicializar
                    initializeTranslatorSession()
                } catch (e: Exception) {
                    Log.e(TAG, "Error inicializando sesiÃ³n: ${e.message}")
                    onError?.invoke("Error de inicializaciÃ³n: ${e.message}")
                }
            }
        }

        @RequiresApi(Build.VERSION_CODES.O)
        override fun onMessage(webSocket: WebSocket, text: String) {
            coroutineScope.launch {
                try {
                    handleMessage(text)
                } catch (e: Exception) {
                    Log.e(TAG, "Error procesando mensaje: ${e.message}")
                }
            }
        }

        @RequiresApi(Build.VERSION_CODES.O)
        override fun onFailure(webSocket: WebSocket, t: Throwable, response: Response?) {
            Log.e(TAG, "Error WebSocket: ${t.message}")
            isConnected.set(false)
            isSessionInitialized.set(false)
            onConnectionStateChanged?.invoke(false)

            // Intentar reconexiÃ³n automÃ¡tica
            if (!isReconnecting.get() && reconnectAttempts < maxReconnectAttempts) {
                attemptReconnection()
            } else {
                onError?.invoke("Error WebSocket: ${t.message}")
            }
        }

        override fun onClosed(webSocket: WebSocket, code: Int, reason: String) {
            Log.d(TAG, "WebSocket cerrado: $code - $reason")
            isConnected.set(false)
            isSessionInitialized.set(false)
            onConnectionStateChanged?.invoke(false)
        }
    }

    // === RECONEXIÃ“N AUTOMÃTICA ===
    @RequiresApi(Build.VERSION_CODES.O)
    private fun attemptReconnection() {
        if (isReconnecting.get()) return

        isReconnecting.set(true)
        coroutineScope.launch {
            try {
                reconnectAttempts++
                Log.d(TAG, "Intentando reconexiÃ³n #$reconnectAttempts")


                if (connect()) {
                    Log.d(TAG, "ReconexiÃ³n exitosa")
                } else {
                    Log.e(TAG, "FallÃ³ reconexiÃ³n #$reconnectAttempts")
                }
            } catch (e: Exception) {
                Log.e(TAG, "Error en reconexiÃ³n: ${e.message}")
            } finally {
                isReconnecting.set(false)
            }
        }
    }

    // === INICIALIZACIÃ“N OPTIMIZADA ===
    private suspend fun initializeTranslatorSession() {
        Log.d(TAG, "Inicializando sesiÃ³n optimizada...")

        val sessionConfig = JSONObject().apply {
            put("type", "session.update")
            put("event_id", "translator_init_${System.currentTimeMillis()}")

            put("session", JSONObject().apply {
                put("modalities", JSONArray().apply {
                    put("text")
                    put("audio")
                })
                put("instructions", translatorInstructions)
                put("voice", "sage") // Voz mÃ¡s rÃ¡pida
                put("input_audio_format", "pcm16")
                put("output_audio_format", "pcm16")
                put("input_audio_transcription", JSONObject().apply {
                    put("model", "whisper-1")
                })
                put("turn_detection", JSONObject().apply {
                    put("type", "server_vad")
                    put("threshold", 0.3) // MÃ¡s sensible
                    put("prefix_padding_ms", 200) // Menos padding
                    put("silence_duration_ms", 500) // Pausa mÃ¡s corta
                })
                put("temperature", 0.6) // Muy conservador para precisiÃ³n
                put("max_response_output_tokens", 1024) // Reducido para velocidad
            })
        }

        if (sendMessage(sessionConfig)) {
            Log.d(TAG, "ConfiguraciÃ³n enviada exitosamente")
        } else {
            Log.e(TAG, "Error enviando configuraciÃ³n")
            onError?.invoke("Error enviando configuraciÃ³n de sesiÃ³n")
        }
    }

    // === MANEJO DE MENSAJES OPTIMIZADO ===
    @RequiresApi(Build.VERSION_CODES.O)
    private suspend fun handleMessage(text: String) {
        try {
            val json = JSONObject(text)
            val type = json.getString("type")

            when (type) {
                "session.created" -> {
                    Log.d(TAG, "SesiÃ³n creada")
                }

                "session.updated" -> {
                    isSessionInitialized.set(true)
                    Log.d(TAG, "Traductor listo âœ“")
                }

                "response.audio.delta" -> {
                    lastAudioReceiveTime = System.currentTimeMillis()
                    val delta = json.getString("delta")
                    try {
                        val audioBytes = Base64.getDecoder().decode(delta)

                        playbackMutex.withLock {
                            playbackBuffer.write(audioBytes)
                            val data = playbackBuffer.toByteArray()

                            // Enviar chunks mÃ¡s pequeÃ±os para menor latencia
                            if (data.size >= 160) { // 10ms de audio
                                onAudioReceived?.invoke(data.copyOf())
                                playbackBuffer.reset()
                            }
                        }
                    } catch (e: Exception) {
                        Log.e(TAG, "Error decodificando audio: ${e.message}")
                    }
                }

                "response.audio.done" -> {
                    playbackMutex.withLock {
                        if (playbackBuffer.size() > 0) {
                            val remainingData = playbackBuffer.toByteArray()
                            onAudioReceived?.invoke(remainingData)
                            playbackBuffer.reset()
                        }
                    }
                    Log.d(TAG, "Audio completado")
                }

                "response.audio_transcript.delta" -> {
                    val delta = json.getString("delta")
                    transcriptionChannel.trySend(delta)
                    Log.d(TAG, "TraducciÃ³n: $delta")
                }

                "input_audio_buffer.speech_started" -> {
                    Log.d(TAG, "ðŸŽ¤ Detectando voz...")
                    playbackMutex.withLock {
                        playbackBuffer.reset()
                    }
                }

                "input_audio_buffer.speech_stopped" -> {
                    Log.d(TAG, "ðŸ”„ Procesando...")
                }

                "conversation.item.input_audio_transcription.completed" -> {
                    val transcript = json.getString("transcript")
                    inputTranscriptionChannel.trySend(transcript)

                    if (isTranslationMode && transcript.isNotBlank()) {
                        processTranscriptionForTranslation(transcript)
                    }

                    Log.d(TAG, "ðŸ“ Original: $transcript")
                }

                "response.created" -> {
                    Log.d(TAG, "âš¡ Generando traducciÃ³n...")
                    lastAudioReceiveTime = System.currentTimeMillis()
                }

                "response.done" -> {
                    Log.d(TAG, "âœ… TraducciÃ³n completada")
                }

                "error" -> {
                    val error = json.getJSONObject("error")
                    val errorMsg = "${error.getString("type")}: ${error.getString("message")}"
                    Log.e(TAG, "âŒ Error: $errorMsg")
                    onError?.invoke("Error: $errorMsg")

                    // Si es error de rate limit, esperar y reintentar
                    if (errorMsg.contains("rate_limit")) {
                        coroutineScope.launch {
                            delay(5000)
                            if (!isConnected.get()) {
                                connect()
                            }
                        }
                    }
                }

                else -> {
                    // Log.d(TAG, "Mensaje: $type") // Comentado para reducir logs
                }
            }
        } catch (e: Exception) {
            Log.e(TAG, "Error parseando mensaje: ${e.message}")
        }
    }

    // === PROCESAMIENTO DE TRADUCCIÃ“N MEJORADO ===
    private fun processTranscriptionForTranslation(transcript: String) {
        if (transcript.isBlank() || transcript.length < 2) return

        try {
            // Detectar idioma mejorado
            val detectedLanguage = detectLanguage(transcript)
            onLanguageDetected?.invoke(detectedLanguage)

            val targetLanguage = when (detectedLanguage.language) {
                "es" -> "ru"
                "ru" -> "es"
                else -> "es"
            }

            Log.d(TAG, "ðŸ” ${detectedLanguage.language} â†’ $targetLanguage")

            // La traducciÃ³n se maneja automÃ¡ticamente por el modelo
        } catch (e: Exception) {
            Log.e(TAG, "Error procesando transcripciÃ³n: ${e.message}")
        }
    }

    // === DETECCIÃ“N DE IDIOMA MEJORADA ===
    private fun detectLanguage(text: String): DetectedLanguage {
        val cyrillicPattern = Regex("[Ð°-ÑÑ‘]", RegexOption.IGNORE_CASE)
        val latinPattern = Regex("[a-zÃ¡Ã©Ã­Ã³ÃºÃ±Ã¼Â¿Â¡]", RegexOption.IGNORE_CASE)

        val hasCyrillic = cyrillicPattern.containsMatchIn(text)
        val hasLatinSpanish = latinPattern.containsMatchIn(text)

        // Palabras clave mÃ¡s completas
        val russianKeywords = listOf(
            "Ð·Ð´Ñ€Ð°Ð²ÑÑ‚Ð²ÑƒÐ¹Ñ‚Ðµ", "Ð¿Ñ€Ð¸Ð²ÐµÑ‚", "ÐºÐ°Ðº", "Ð´ÐµÐ»Ð°", "ÑÐ¿Ð°ÑÐ¸Ð±Ð¾", "Ð¿Ð¾Ð¶Ð°Ð»ÑƒÐ¹ÑÑ‚Ð°",
            "Ð´Ð°", "Ð½ÐµÑ‚", "Ñ…Ð¾Ñ€Ð¾ÑˆÐ¾", "Ð¿Ð»Ð¾Ñ…Ð¾", "Ñ‡Ñ‚Ð¾", "Ð³Ð´Ðµ", "ÐºÐ¾Ð³Ð´Ð°", "Ð¿Ð¾Ñ‡ÐµÐ¼Ñƒ",
            "ÑÑ‡Ñ‘Ñ‚", "Ð±Ð°Ð»Ð°Ð½Ñ", "Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒ", "Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ð°", "Ð²Ð¾Ð¿Ñ€Ð¾Ñ"
        )

        val spanishKeywords = listOf(
            "hola", "buenos", "dÃ­as", "cÃ³mo", "estÃ¡", "gracias", "por", "favor",
            "sÃ­", "no", "bien", "mal", "quÃ©", "dÃ³nde", "cuÃ¡ndo", "por", "quÃ©",
            "cuenta", "saldo", "ayuda", "problema", "pregunta"
        )

        val textLower = text.toLowerCase()
        val russianMatches = russianKeywords.count { textLower.contains(it) }
        val spanishMatches = spanishKeywords.count { textLower.contains(it) }

        return when {
            hasCyrillic || russianMatches > spanishMatches -> {
                val confidence = if (hasCyrillic) 0.95f else (russianMatches * 0.2f + 0.6f).coerceAtMost(0.9f)
                DetectedLanguage("ru", confidence, text)
            }
            hasLatinSpanish || spanishMatches > russianMatches -> {
                val confidence = if (hasLatinSpanish) 0.85f else (spanishMatches * 0.2f + 0.6f).coerceAtMost(0.8f)
                DetectedLanguage("es", confidence, text)
            }
            else -> DetectedLanguage("es", 0.6f, text)
        }
    }

    // === AUDIO STREAMING OPTIMIZADO ===
    private fun startAudioTimeoutWatcher() {
        coroutineScope.launch {
            while (isConnected.get()) {
                val currentTime = System.currentTimeMillis()
                if (lastAudioReceiveTime > 0 &&
                    currentTime - lastAudioReceiveTime > audioTimeoutMs) {

                    playbackMutex.withLock {
                        if (playbackBuffer.size() > 0) {
                            Log.d(TAG, "â° Enviando audio restante")
                            val remainingData = playbackBuffer.toByteArray()
                            onAudioReceived?.invoke(remainingData)
                            playbackBuffer.reset()
                        }
                    }
                    lastAudioReceiveTime = 0L
                }
                delay(300) // MÃ¡s frecuente
            }
        }
    }

    @RequiresApi(Build.VERSION_CODES.O)
    private fun startAudioStreamingLoop() {
        coroutineScope.launch {
            while (isConnected.get()) {
                try {
                    if (isSessionInitialized.get()) {
                        processAndSendAudioBuffer()
                    }
                    delay(audioSendInterval)
                } catch (e: Exception) {
                    Log.e(TAG, "Error en loop de audio: ${e.message}")
                    delay(1000)
                }
            }
        }
    }

    @RequiresApi(Build.VERSION_CODES.O)
    private suspend fun processAndSendAudioBuffer() {
        if (!isSessionInitialized.get()) return

        val audioChunks = audioBufferMutex.withLock {
            if (audioBuffer.isNotEmpty()) {
                val chunks = audioBuffer.toList()
                audioBuffer.clear()
                chunks
            } else {
                emptyList()
            }
        }

        if (audioChunks.isNotEmpty()) {
            val combinedAudio = combineAudioChunks(audioChunks)
            if (combinedAudio.size >= minAudioChunkSize) {
                sendAudioToOpenAI(combinedAudio)
            }
        }
    }

    private fun combineAudioChunks(chunks: List<ByteArray>): ByteArray {
        val totalSize = chunks.sumOf { it.size }
        val finalSize = minOf(totalSize, maxAudioChunkSize)

        val result = ByteArray(finalSize)
        var offset = 0

        for (chunk in chunks) {
            val remainingSpace = finalSize - offset
            if (remainingSpace <= 0) break

            val copySize = minOf(chunk.size, remainingSpace)
            System.arraycopy(chunk, 0, result, offset, copySize)
            offset += copySize
        }

        return result
    }

    @RequiresApi(Build.VERSION_CODES.O)
    suspend fun sendAudioToOpenAI(audioData: ByteArray) {
        try {
            val base64Audio = Base64.getEncoder().encodeToString(audioData)
            val message = JSONObject().apply {
                put("type", "input_audio_buffer.append")
                put("event_id", "audio_${System.currentTimeMillis()}")
                put("audio", base64Audio)
            }

            sendMessage(message)
        } catch (e: Exception) {
            Log.e(TAG, "Error enviando audio: ${e.message}")
        }
    }

    suspend fun generateTranslation() {
        try {
            val message = JSONObject().apply {
                put("type", "response.create")
                put("event_id", "translate_${System.currentTimeMillis()}")
                put("response", JSONObject().apply {
                    put("modalities", JSONArray().apply {
                        put("text")
                        put("audio")
                    })
                })
            }
            sendMessage(message)
            Log.d(TAG, "ðŸš€ Solicitando traducciÃ³n")
        } catch (e: Exception) {
            Log.e(TAG, "Error solicitando traducciÃ³n: ${e.message}")
        }
    }

    // === MÃ‰TODOS PÃšBLICOS ===
    suspend fun addAudioData(audioData: ByteArray) {
        if (!isConnected.get() || !isTranslationMode) return
        audioBufferMutex.withLock {
            audioBuffer.add(audioData)
        }
    }

    fun getAudioResponseChannel(): ReceiveChannel<ByteArray> = audioResponseChannel
    fun getTranscriptionChannel(): ReceiveChannel<String> = transcriptionChannel
    fun getInputTranscriptionChannel(): ReceiveChannel<String> = inputTranscriptionChannel
    fun getTranslationChannel(): ReceiveChannel<TranslationResult> = translationChannel

    fun isConnected(): Boolean = isConnected.get()
    fun isSessionReady(): Boolean = isSessionInitialized.get()
    fun isTranslating(): Boolean = isTranslationMode

    private suspend fun sendMessage(jsonObject: JSONObject): Boolean {
        return try {
            val jsonString = jsonObject.toString()
            val success = webSocket?.send(jsonString) ?: false
            if (!success) {
                Log.e(TAG, "Error enviando mensaje WebSocket")
            }
            success
        } catch (e: Exception) {
            Log.e(TAG, "Error enviando mensaje: ${e.message}")
            false
        }
    }

    suspend fun disconnect() {
        Log.d(TAG, "ðŸ”Œ Desconectando...")
        isConnected.set(false)
        isSessionInitialized.set(false)

        try {
            webSocket?.close(1000, "MCN Translator disconnect")
        } catch (e: Exception) {
            Log.e(TAG, "Error cerrando WebSocket: ${e.message}")
        }

        coroutineScope.cancel()
        audioResponseChannel.close()
        transcriptionChannel.close()
        translationChannel.close()
        inputTranscriptionChannel.close()

        audioBufferMutex.withLock {
            audioBuffer.clear()
        }

        playbackMutex.withLock {
            playbackBuffer.reset()
        }

        Log.d(TAG, "âœ… Desconectado")
    }
}
